{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/agods/blob/main/day5/4.feature_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a14028cd-85ae-493d-af64-f35112d0c53d"
      },
      "source": [
        "<img src=\"https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/agods/nyp_ago_logo.png\" width='400'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLwc58I8N7Iu"
      },
      "source": [
        "# Feature Selection\n",
        "\n",
        "In this lab, you will learn:\n",
        "- different methods to do feature selection\n",
        "- differences in statistical approach and machine learning approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHTh1nNvN7Ix"
      },
      "source": [
        "## Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnSx44U5N7Ix"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIoA71pPN7I3"
      },
      "source": [
        "## Create the Data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggTsRk1MN7I4"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "boston = pd.read_csv('data/boston.csv', index_col=0)\n",
        "X = boston.drop('MEDV', axis=1)\n",
        "y = boston['MEDV']\n",
        "\n",
        "# Split the data into train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Reconstruct dataframe\n",
        "X_train = pd.DataFrame(X_train, columns=scaler.get_feature_names_out())\n",
        "X_test = pd.DataFrame(X_test, columns=scaler.get_feature_names_out())\n",
        "\n",
        "# Create degree 2 polynomial features \n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "\n",
        "X_train = poly.fit_transform(X_train)\n",
        "X_test = poly.transform(X_test)\n",
        "\n",
        "X_train = pd.DataFrame(X_train, columns=poly.get_feature_names_out())\n",
        "X_test = pd.DataFrame(X_test, columns=poly.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5VAO9UuN7I4"
      },
      "source": [
        "Here we compute the $R^2$ score (for easier comparison) of linear regression for both train and test score too see if there is overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XygwngFoN7I4"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))\n",
        "print(\"Number of features used: {}\".format(np.sum(lr.coef_ != 0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjHdE9XU0EMK"
      },
      "source": [
        "We can see that the model is overfitting.  One way to fight overfitting is to use regularized model. Lasso is known to penalize the model by driving the coefficients down to 0, in a way simplify the model. This is like automatic feature selection.\n",
        "\n",
        "Let's try using Lasso and find the best alpha by doing a grid search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFXvtq_3N7I7"
      },
      "outputs": [],
      "source": [
        "## Write your code here\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import Lasso \n",
        "\n",
        "alphas = np.arange(0.001, 0.01, 0.001)\n",
        "print('the alphas:', alphas)\n",
        "\n",
        "param_grid = [{'alpha': alphas}]\n",
        "lasso = Lasso(selection='random', max_iter=50000)\n",
        "grid_cv = GridSearchCV(lasso,\n",
        "             param_grid,\n",
        "             cv=5, \n",
        "             scoring='neg_root_mean_squared_error',\n",
        "             return_train_score=True)\n",
        "\n",
        "grid_cv.fit(X_train, y_train)\n",
        "\n",
        "best_estimator = grid_cv.best_estimator_\n",
        "print(\"Training set score: {:.2f}\".format(best_estimator.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.2f}\".format(best_estimator.score(X_test, y_test)))\n",
        "print(\"Number of features used: {}\".format(np.sum(best_estimator.coef_ != 0)))\n",
        "\n",
        "\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZmGG5RH0EML"
      },
      "source": [
        "Notice that the number of coefficients have been reduced to 81, in other words lasso do a automatic feature selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwtceLem0EML"
      },
      "source": [
        "Let's try using scikit learn's feature selection algorithm, e.g. Recursiv Feature Elimination. For comparison, let's also restrict the number of features to 81."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvxwM7nQ0EML"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR \n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "estimator = SVR(kernel=\"linear\")\n",
        "selector = RFE(estimator, n_features_to_select=81, step=1)\n",
        "X_train_rfe = selector.fit_transform(X_train, y_train)\n",
        "X_train_rfe = pd.DataFrame(X_train_rfe, columns=selector.get_feature_names_out())\n",
        "X_test_rfe = selector.transform(X_test)\n",
        "X_test_rfe = pd.DataFrame(X_test_rfe, columns=selector.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb4svqAI0EML"
      },
      "source": [
        "Using the selected feature, we fit the features with a normal Linear Regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z60Ckqrd0EMM"
      },
      "outputs": [],
      "source": [
        "lr_rfe = LinearRegression()\n",
        "lr_rfe.fit(X_train_rfe, y_train)\n",
        "\n",
        "print(\"Training set score: {:.2f}\".format(lr_rfe.score(X_train_rfe, y_train)))\n",
        "print(\"Test set score: {:.2f}\".format(lr_rfe.score(X_test_rfe, y_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI8kuxqo0EMM"
      },
      "source": [
        "Let's now instead using statistical method to select coefficients based on it's fitted p-values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_ZPV3Eb0EMM"
      },
      "outputs": [],
      "source": [
        "len(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJr7LCHR0EMM"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "model = sm.OLS(y_train.values, X_train)\n",
        "\n",
        "# Fit your model to your training set\n",
        "result = model.fit()\n",
        "\n",
        "# Print summary statistics of the model's performance\n",
        "result.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtBTOp9K0EMM"
      },
      "source": [
        "We will now only use those coefficients which has p-value <= 0.05."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlERJxP50EMM"
      },
      "outputs": [],
      "source": [
        "columns = result.pvalues[result.pvalues <= 0.05].index\n",
        "\n",
        "X_train_stat = X_train[columns]\n",
        "X_test_stat = X_test[columns]\n",
        "\n",
        "lr_stat = LinearRegression()\n",
        "lr_stat.fit(X_train_stat, y_train)\n",
        "\n",
        "print(lr_stat.score(X_train_stat, y_train))\n",
        "print(lr_stat.score(X_test_stat, y_test))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Ex1_Regularization.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}