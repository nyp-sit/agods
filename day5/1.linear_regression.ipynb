{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a14028cd-85ae-493d-af64-f35112d0c53d"
   },
   "source": [
    "<img src=\"https://nyp-aicourse.s3.ap-southeast-1.amazonaws.com/agods/nyp_ago_logo.png\" width='400'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lks9-NryXjI7"
   },
   "source": [
    "\n",
    "# Practical: Linear Regression\n",
    "\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Learn how use scatter plot to visualize relationship between two variables.\n",
    "- Use matplotlib to draw a best-fit line.\n",
    "- Using Scikit-Learn to generate a Linear Regression model and perform prediction.\n",
    "- Evaluate the model by looking at the mean squared error (MSE) value, R2 and adjusted R2 values.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The linear regression is one of the most common algorithms used to model the relationship among variables. It is routinely used to predict a numerical outcome from a related set of input predictors.\n",
    "\n",
    "The simplest form of linear regression involves two variables where one variable is used to predict another. The assumption is that the two variables have a linear relationship. This can be expressed as an equation below, where we wish to predict y given a known value of x.\n",
    "\n",
    "$$y = β_0+β_1x$$\n",
    "\n",
    "In the equation, the values of $β_0$ and $β_1$ is fixed and modelling  refers to the processing of determining the values of $β_0$ and $β_1$.\n",
    "\n",
    "Theorectically, for a linear relationship (straight line in a cartesian plane), we will only need 2 sets of (x, y) values (2 points) but in practise, due to noise and errors, we will usually need more. The usefulness of the equation depends on how well the values of $β_0$ and $β_1$ are chosen. That is of course possible only if we have sufficient and quality data from which to derive the best $β$ values. \n",
    "\n",
    "## Simple Linear Regression\n",
    "\n",
    "In this practical, we will see how to use data to generate a linear regression models and then use the model for prediction. We will use a simple set of data for illustration purposes.\n",
    "\n",
    "### Get the data\n",
    "\n",
    "Read in the file 'mortgage_data.csv' in data folder as a pandas dataframe.\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('data/mortgage_data.csv', index_col=\"Year\")\n",
    "print(df.head())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJAq2HE6XjJB"
   },
   "outputs": [],
   "source": [
    "# Enter your codes here to read in the SimpleLinearRegressionData.csv file.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('data/mortgage_data.csv', index_col=\"Year\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCB2k1qMXjJC"
   },
   "source": [
    "The codes uses the ```read_csv()``` function to read data stored in a CSV file. It uses the ```Year``` column as the index column. The data is stored in a Pandas ```DataFrame``` named ```df```.\n",
    "\n",
    "The print statement will print out the first few rows of data. You should be able to see the following:\n",
    "\n",
    "![mortgage_samples](images/mortgage_samples.png)\n",
    "\n",
    "As can be seen, the data consists of the the columns *Year* (now set as the index of the ```DataFrame```), *Rates* and *Mortage*.\n",
    "Usually, it is useful to visualize the relationship between two variables using a scatter plot.\n",
    "\n",
    "### Visualize the relationship\n",
    "\n",
    "Use a scatter plot as shown in the codes below:\n",
    "\n",
    "```python\n",
    "df.plot.scatter(title=\"Plot of Rates vs Mortgages\", x=\"Rates\", y=\"Mortgage\", color=\"red\")\n",
    "```\n",
    "\n",
    "Run the codes and you should see a scatter plot as shown below:\n",
    "\n",
    "<img src=\"images/scatter_plot.png\" width=60% />\n",
    "\n",
    "As can be seen from the figure, the two variables *Rates* and *Mortgages* are somewhat related to each other in a linear but inversely proportional manner. This implies that we should be able to get a good linear regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAVkEwv7XjJD"
   },
   "outputs": [],
   "source": [
    "# Enter codes to visualize the data using a scatterplot\n",
    "\n",
    "#df.plot.scatter(title=\"Plot of Rates vs Mortgages\", x=\"Rates\", y=\"Mortgage\", color=\"red\")\n",
    "sns.scatterplot(data=df, x=\"Rates\", y=\"Mortgage\", color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fF1ak1tXjJD"
   },
   "source": [
    "### Best-Fit Line\n",
    "Linear regression works by deriving a mathematical equation from the data. Due to inaccuracy and noise in the data, a perfect line is near impossible. Our aim is to plot a line that best fit the set of data. The best-fit line is a line that minimizes residual errors. Residual error is the difference between the observed and the predicted values.\n",
    "\n",
    "Numpy’s [`polyfit()`](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html) generates the coefficients of the best-fit line in the form of a vector. The coefficients are then passed into the [`poly1d()`](https://numpy.org/doc/stable/reference/generated/numpy.poly1d.html) function to create the best-fit line. We can then plot the best fit line. \n",
    "\n",
    "Plot the best-fit line, insert the following codes:\n",
    "```python\n",
    "#Use Pandas to create a scatter plot, x-axis is Rates and y-axis is Mortage\n",
    "#Use red colour for the points\n",
    "ax = df.plot.scatter(x=\"Rates\", y=\"Mortgage\", color=\"red\")\n",
    "\n",
    "#polyfit = Fit the data using the least square polynomial\n",
    "#Returns a list of the coefficients\n",
    "coefficients = np.polyfit(df[\"Rates\"], df[\"Mortgage\"], 1)\n",
    "#Use the coefficients to create a polynomial\n",
    "p = np.poly1d(coefficients)\n",
    "#Evaluate the polynomial on the rates data\n",
    "df[\"best_fit\"] = p(df.loc[:, \"Rates\"])\n",
    "#Create another dataframe with rates as the index\n",
    "# and Mortage vs the best_fit points\n",
    "df2 = df.set_index(\"Rates\", inplace=False)\n",
    "#Plot\n",
    "df2.best_fit.plot(ax=ax)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2y1ftpcdXjJE"
   },
   "outputs": [],
   "source": [
    "# Enter code to plot the best-fit line here\n",
    "\n",
    "# Use Pandas or seaborn to create a scatter plot, x-axis is Rates and y-axis is Mortage\n",
    "# Use red colour for the points\n",
    "\n",
    "# df.plot.scatter(x=\"Rates\", y=\"Mortgage\", color=\"red\")\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.scatterplot(data=df, x=\"Rates\", y=\"Mortgage\", color=\"red\")\n",
    "\n",
    "# Use polyfit to find the coefficients of the best fit line (of degree 1 polynomial) \n",
    "coefficients = np.polyfit(df[\"Rates\"], df[\"Mortgage\"], 1)\n",
    "\n",
    "# Use the coefficients to create a polynomial function\n",
    "polynomial = np.poly1d(coefficients)\n",
    "\n",
    "# Use the function to generate the y values for different x values (the rates)\n",
    "x_values = df[\"Rates\"].values\n",
    "y_values = polynomial(x_values)\n",
    "\n",
    "# Now plot a line plot based on x and y\n",
    "sns.lineplot(x=x_values, y=y_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tR863fhVXjJE"
   },
   "source": [
    "### Simple Linear Regression Modelling\n",
    "\n",
    "We will now generate a scikit-learn linear regression model and subsequently use the model to predict a value. The Linear Regression algorithm is found under the [`sklearn.linear_model`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) module.\n",
    "\n",
    "Create a LinearRegression model and train the model using data from the *Rates* and *Mortage* columns as follows:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#Get the rate column and reshape it to a series\n",
    "rates = df[\"Rates\"].values.reshape(-1, 1)\n",
    "#Get the mortgage column and reshape it to a series\n",
    "mortgage = df[\"Mortgage\"].values.reshape(-1, 1)\n",
    "#Create LinearRegression\n",
    "model = LinearRegression()\n",
    "#train the model using the fit() function\n",
    "model.fit(rates, mortgage)\n",
    "```\n",
    "\n",
    "**Note**\n",
    "\n",
    "Scikit-learn model expects X (the features) to be of the shape (n_samples, n_features).  So our rates, which is a single dimensional array of shape (n_samples), need to be reshaped to (n_samples, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJZkMSpjXjJF"
   },
   "outputs": [],
   "source": [
    "# Enter your codes here\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "rates = df[\"Rates\"].values.reshape(-1,1)\n",
    "print(rates.shape)\n",
    "mortgage = df['Mortgage'].values\n",
    "print(mortgage.shape)\n",
    "\n",
    "# Create LinearRegression\n",
    "model = LinearRegression()\n",
    "\n",
    "# train the model using the fit() function\n",
    "model.fit(rates, mortgage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTSy1f3zXjJG"
   },
   "source": [
    "We can also take a look at the equation generated by the ```fit()``` function. The coefficient ($β_1$) and intercept ($β_0$) can be retrieved using ```model.coef_``` and ```model.intercept_```.\n",
    "\n",
    "Print out the equation of the regression model as follows:\n",
    "\n",
    "```python\n",
    "print(\"Equation y={0:.2f}*x + {1:.2f}\".format(model.coef_[0], model.intercept_))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cEojN1UzXjJG"
   },
   "outputs": [],
   "source": [
    "#Print our the model coefficients and intercept there\n",
    "\n",
    "print(\"Equation y={0:.2f}*x + {1:.2f}\".format(model.coef_[0], model.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIktlnvLXjJH"
   },
   "source": [
    "You should see the following results:\n",
    "\n",
    "```\n",
    "Equation y=-32518.17*x + 501105.72\n",
    "```\n",
    "\n",
    "This is the mathematical model that can be used for subsequent prediction using the ```predict()``` function.\n",
    "To see how prediction works, we will input a value of 8.0 for the rates and see how well the model works. Based on the best-fit line we plotted earlier, we can guess that the predicted valued will be between 225k and 250k.\n",
    "\n",
    "Key in the following codes and execute them to see the results for predicting mortage at a rate of 8%\n",
    "\n",
    "```python\n",
    "# Create a test value for rate of 8.0\n",
    "test_rate = np.array([[8.0]])\n",
    "\n",
    "# Use the predict function of the model to perform the prediction\n",
    "predicted_mortgage = model.predict(test_rate)[0]\n",
    "print(predicted_mortgage)\n",
    "\n",
    "# Print out the predicted mortgage value\n",
    "print(\"Predicted mortgage: {0:.2f}\".format(predicted_mortgage))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YbPw4SxXjJH"
   },
   "outputs": [],
   "source": [
    "# Enter codes to perform prediction using your Regression model\n",
    "\n",
    "# Create a test value for rate of 8.0\n",
    "test_rate = np.array([[8.0]])\n",
    "\n",
    "# Use the predict function of the model to perform the prediction\n",
    "predicted_mortgage = model.predict(test_rate)[0]\n",
    "print(predicted_mortgage)\n",
    "\n",
    "# Print out the predicted mortgage value\n",
    "print(\"Predicted mortgage: {0:.2f}\".format(predicted_mortgage))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTnhT0aqXjJI"
   },
   "source": [
    "You should see the following output:\n",
    "\n",
    "```\n",
    "Predicted mortgage: 240960.40\n",
    "```\n",
    "\n",
    "As can be seen from the best-fit line, the value is approximately 240k which is inline with the prediction. You can also verify the value by substituting the value in the equation generated previously.\n",
    "\n",
    "### Evaluating the Model\n",
    "\n",
    "Very often, we will need to know how well our model works, especially when we need to compare different models and pick the best among them.\n",
    "\n",
    "#### Mean Square Error (MSE)\n",
    "\n",
    "The Mean Square Error value provides an indication of the performance of a model. Recall that we generate a model by minimizing the MSE of the training data, in other words, the lower the value of MSE, the smaller the prediction error.\n",
    "\n",
    "To generate the MSE value of our model, we can do the following:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculates the MSE value\n",
    "mse = mean_squared_error(mortgage, model.predict(rates))\n",
    "\n",
    "# Print out the SSE value\n",
    "print(\"SSE: {0:.2f}\".format(rss))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UU8SXqCGXjJI"
   },
   "outputs": [],
   "source": [
    "# Enter codes to calculate the MSE value of your model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculates the MSE value\n",
    "mse = mean_squared_error(mortgage, model.predict(rates))\n",
    "\n",
    "# Print out the MSE value\n",
    "print(\"MSE: {0:.2f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3r0RJTB1XjJI"
   },
   "source": [
    "#### Coefficient of Determination ($R^2$) Value\n",
    "\n",
    "$R^2$ value is an important and commonly used value to compare the predictive power of the models. ```Scikit-Learn``` package provides a ```r2_score()``` function under ```sklearn.metrics``` that can helps to calculate the $R^2$ value.\n",
    "\n",
    "\n",
    "```python\n",
    "# import the r2_score function\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Calculates the R sequared value\n",
    "r2 = r2_score(mortgage, model.predict(rates))\n",
    "# Print out the R sequared value\n",
    "print(\"R Squared: {0:.2f}\".format(r2))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Uo2A8YvXjJJ"
   },
   "outputs": [],
   "source": [
    "# Enter the codes to compute R^2\n",
    "\n",
    "# import the r2_score function\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Calculates the R sequared value\n",
    "r2 = r2_score(mortgage, model.predict(rates))\n",
    "# Print out the R sequared value\n",
    "print(\"R Squared: {0:.2f}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_klypFgXjJM"
   },
   "source": [
    "Values of $R^2$ ranges from 0 (worst) to 1 (best). A score of 0.82 is very good performance for our model. This is not unexpected as the scatter plot already shown us that the variables are linearly related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDZQA7HXXjJN"
   },
   "source": [
    "## Exercise: Multiple Linear Regression\n",
    "\n",
    "In most real-world problems, we will need to deal with more than one input variables. In such cases, we can use a more generalized equation:\n",
    "\n",
    "$$y=β_0+β_1 x_1+β_2 x_2+⋯+β_kx_k$$\n",
    "Where $k$ is the number of input variables/predictors.\n",
    "\n",
    "Let us now extend the use of scikit-learn regression model from single to three input variables. We will be using a dataset containing insurance claims for a single medical treatment performed in a hospital. In addition to the claim amount (CLAIM), the data file also contains patient age (AGE), length of hospital stay (LOS) and a severity of illness category (ASG). The ASG field is based on several health measures and higher scores indicate greater severity of the illness. \n",
    "\n",
    "In this exercise, you are required to build a regression model that predicts the total claim amount for a patient based on his/her length of stay, severity of illness and patient age.\n",
    "Use the codes you have done in the previous section and the following task list as a guide:\n",
    "\n",
    "1. Read the insurance_claim.csv file in the data folder\n",
    "2. Create a LinearRegression model.\n",
    "3. Train the model using the `fit()` function.\n",
    "4. Use the model to predict and print out a predicted claim value.\n",
    "5. Print out the regression equation.\n",
    "6. Print out $R^2$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/insurance_claim.csv\")\n",
    "df[['ASG','AGE','LOS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['ASG','AGE','LOS']]\n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7rLES70XjJN"
   },
   "outputs": [],
   "source": [
    "#Enter your answers here\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df = pd.read_csv(\"data/insurance_claim.csv\")\n",
    "X = df[['ASG','AGE','LOS']].values\n",
    "y = df['CLAIM'].values\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "test_data = test_data = np.array([\n",
    "    [0, 26, 2]\n",
    "])\n",
    "\n",
    "predicted_claim = model.predict(test_data)[0]\n",
    "print(\"Predicted claim is: {0:.2f} \".format(predicted_claim))\n",
    "coeff = model.coef_\n",
    "print(\"Equation: {0:.2f} * ASG + {1:.2f} * AGE + {2:.2f} * LOS + {3:.2f}\".format(coeff[0], coeff[1], coeff[2], model.intercept_))\n",
    "r2 = r2_score(y, model.predict(X))\n",
    "print(\"R Squared: {0:.2f}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhgR4fqyXjJN",
    "raw_mimetype": "text/html",
    "tags": [
     "hidecode"
    ]
   },
   "source": [
    "\n",
    "<details>\n",
    "    <summary>Click here for answer</summary>\n",
    "\n",
    "```\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df = pd.read_csv(\"data/insurance_claim.csv\")\n",
    "X = df[['ASG','AGE','LOS']].values\n",
    "y = df['CLAIM'].values\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "test_data = test_data = np.array([\n",
    "    [0, 26, 2]\n",
    "])\n",
    "\n",
    "predicted_claim = model.predict(test_data)[0]\n",
    "print(\"Predicted claim is: {0:.2f} \".format(predicted_claim))\n",
    "coeff = model.coef_\n",
    "print(\"Equation: {0:.2f} * ASG + {1:.2f} * AGE + {2:.2f} * LOS + {3:.2f}\".format(coeff[0], coeff[1], coeff[2], model.intercept_))\n",
    "r2 = r2_score(y, model.predict(X))\n",
    "print(\"R Squared: {0:.2f}\".format(r2))\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bc2KJp1iXjJO"
   },
   "source": [
    "#### Adjusted R2\n",
    "\n",
    "In some cases, $R^2$ does not provide the best evaluation measurement of the performance of our model. This is because $R^2$ measurement does not penalize the inclusion of useless input variables. In other words, the more input variables used in the model, the higher the score. This is not desireable as including input variables that does not contributes significantly to quality of the prediction adds costs for data collection as well as processing time. \n",
    "\n",
    "It is thus useful to use Adjusted $R^2$ defined as:\n",
    "\n",
    "$$Adjusted\\, R^2 = 1 - \\frac{(1-R^2)(n-1)}{n-p-1}$$\n",
    "\n",
    "Where $n$ = number of data samples in the data and $p$ = number of input variables\n",
    "As can be seen from the equation, as p increases, adjusted $R^2$ value decreases (the larger the value of $R^2$, the better the model).\n",
    "The following codes calculates the adjusted $R^2$ value from the $R^2$ we obtained previously from the ```r2_score()``` function.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "X = df[['ASG','AGE','LOS']].values\n",
    "y = df['CLAIM'].values\n",
    "r2 = r2_score(y, model.predict(X))\n",
    "\n",
    "num_variables = X.shape[1]\n",
    "num_samples = X.shape[0]\n",
    "\n",
    "adjusted_r2 = 1 - ((1-r2) * (num_samples-1)) / (num_samples-num_variables-1)\n",
    "print(adjusted_r2)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7eTXvzw7XjJO"
   },
   "outputs": [],
   "source": [
    "# Try it out, enter your codes here to calculate the Adjusted R2 value\n",
    "\n",
    "X = df[['ASG','AGE','LOS']].values\n",
    "y = df['CLAIM'].values\n",
    "r2 = r2_score(y, model.predict(X))\n",
    "\n",
    "num_variables = X.shape[1]\n",
    "num_samples = X.shape[0]\n",
    "\n",
    "adjusted_r2 = 1 - ((1-r2) * (num_samples-1)) / (num_samples-num_variables-1)\n",
    "print(adjusted_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKUV5_BzXjJO"
   },
   "source": [
    "If you run the codes, you should see that $R^2$ value is 0.32 while adjusted $R^2$ value is 0.31, showing that adjusted $R^2$ value is less as it takes into account the number of input variables used. Note also that $R^2$ and adjusted $R^2$ values will be very similar if the number of data samples is much larger then the number of input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = []\n",
    "\n",
    "prefixes = ['X', 'Y', 'P']\n",
    "\n",
    "for prefix in prefixes: \n",
    "    for i in range(1,17): \n",
    "        col_name = prefix + str(i)\n",
    "        column_names.append(col_name)\n",
    "\n",
    "column_names.append('Powerall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tasmania = pd.read_csv('Tasmania_Data.csv', header=0, names=column_names)\n",
    "df_adelaide = pd.read_csv('Adelaide_Data.csv', header=0, names=column_names)\n",
    "df_perth = pd.read_csv('Perth_Data.csv', header=0, names=column_names)\n",
    "df_sydney = pd.read_csv('Sydney_Data.csv', header=0, names=column_names)\n",
    "df_tasmania2 = pd.read_csv('Tasmania_Data.csv', header=0, names=column_names)\n",
    "df_adelaide2 = pd.read_csv('Adelaide_Data.csv', header=0, names=column_names)\n",
    "df_perth2 = pd.read_csv('Perth_Data.csv', header=0, names=column_names)\n",
    "df_sydney2 = pd.read_csv('Sydney_Data.csv', header=0, names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_tasmania, df_adelaide, df_perth, df_sydney,df_tasmania2, df_adelaide2, df_perth2, df_sydney2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 575990 entries, 0 to 575989\n",
      "Data columns (total 49 columns):\n",
      " #   Column    Non-Null Count   Dtype  \n",
      "---  ------    --------------   -----  \n",
      " 0   X1        575990 non-null  float64\n",
      " 1   X2        575990 non-null  float64\n",
      " 2   X3        575990 non-null  float64\n",
      " 3   X4        575990 non-null  float64\n",
      " 4   X5        575990 non-null  float64\n",
      " 5   X6        575990 non-null  float64\n",
      " 6   X7        575990 non-null  float64\n",
      " 7   X8        575990 non-null  float64\n",
      " 8   X9        575990 non-null  float64\n",
      " 9   X10       575990 non-null  float64\n",
      " 10  X11       575990 non-null  float64\n",
      " 11  X12       575990 non-null  float64\n",
      " 12  X13       575990 non-null  float64\n",
      " 13  X14       575990 non-null  float64\n",
      " 14  X15       575990 non-null  float64\n",
      " 15  X16       575990 non-null  float64\n",
      " 16  Y1        575990 non-null  float64\n",
      " 17  Y2        575990 non-null  float64\n",
      " 18  Y3        575990 non-null  float64\n",
      " 19  Y4        575990 non-null  float64\n",
      " 20  Y5        575990 non-null  float64\n",
      " 21  Y6        575990 non-null  float64\n",
      " 22  Y7        575990 non-null  float64\n",
      " 23  Y8        575990 non-null  float64\n",
      " 24  Y9        575990 non-null  float64\n",
      " 25  Y10       575990 non-null  float64\n",
      " 26  Y11       575990 non-null  float64\n",
      " 27  Y12       575990 non-null  float64\n",
      " 28  Y13       575990 non-null  float64\n",
      " 29  Y14       575990 non-null  float64\n",
      " 30  Y15       575990 non-null  float64\n",
      " 31  Y16       575990 non-null  float64\n",
      " 32  P1        575990 non-null  float64\n",
      " 33  P2        575990 non-null  float64\n",
      " 34  P3        575990 non-null  float64\n",
      " 35  P4        575990 non-null  float64\n",
      " 36  P5        575990 non-null  float64\n",
      " 37  P6        575990 non-null  float64\n",
      " 38  P7        575990 non-null  float64\n",
      " 39  P8        575990 non-null  float64\n",
      " 40  P9        575990 non-null  float64\n",
      " 41  P10       575990 non-null  float64\n",
      " 42  P11       575990 non-null  float64\n",
      " 43  P12       575990 non-null  float64\n",
      " 44  P13       575990 non-null  float64\n",
      " 45  P14       575990 non-null  float64\n",
      " 46  P15       575990 non-null  float64\n",
      " 47  P16       575990 non-null  float64\n",
      " 48  Powerall  575990 non-null  float64\n",
      "dtypes: float64(49)\n",
      "memory usage: 215.3 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1          5.660000e+02\n",
       "X2          5.660000e+02\n",
       "X3          5.660000e+02\n",
       "X4          5.660000e+02\n",
       "X5          5.660000e+02\n",
       "X6          5.660000e+02\n",
       "X7          5.660000e+02\n",
       "X8          5.660000e+02\n",
       "X9          5.660000e+02\n",
       "X10         5.660000e+02\n",
       "X11         5.660000e+02\n",
       "X12         5.660000e+02\n",
       "X13         5.660000e+02\n",
       "X14         5.660000e+02\n",
       "X15         5.660000e+02\n",
       "X16         5.660000e+02\n",
       "Y1          5.660000e+02\n",
       "Y2          5.660000e+02\n",
       "Y3          5.660000e+02\n",
       "Y4          5.660000e+02\n",
       "Y5          5.660000e+02\n",
       "Y6          5.660000e+02\n",
       "Y7          5.660000e+02\n",
       "Y8          5.660000e+02\n",
       "Y9          5.660000e+02\n",
       "Y10         5.660000e+02\n",
       "Y11         5.660000e+02\n",
       "Y12         5.660008e+02\n",
       "Y13         5.660000e+02\n",
       "Y14         5.660000e+02\n",
       "Y15         5.660000e+02\n",
       "Y16         5.660000e+02\n",
       "P1          2.839113e+05\n",
       "P2          2.818797e+05\n",
       "P3          2.828941e+05\n",
       "P4          2.827192e+05\n",
       "P5          2.825292e+05\n",
       "P6          2.813671e+05\n",
       "P7          2.827483e+05\n",
       "P8          2.821438e+05\n",
       "P9          2.798248e+05\n",
       "P10         2.835486e+05\n",
       "P11         2.814574e+05\n",
       "P12         2.817894e+05\n",
       "P13         2.819688e+05\n",
       "P14         2.862791e+05\n",
       "P15         2.821897e+05\n",
       "P16         2.838756e+05\n",
       "Powerall    4.241838e+06\n",
       "dtype: float64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = df.drop('Powerall', axis=1)\n",
    "ytrain = df['Powerall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # x, y = make_regression(n_samples=100000, n_features=500)\n",
    "# xtrain, xtest, ytrain, ytest=train_test_split(x, y, test_size=0.15)\n",
    "sgdr = SGDRegressor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.6782969273496875"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.501372474509713"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 104.75 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "30min 12s ± 1h 8min 39s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\markk\\miniconda3\\envs\\agodsenv\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%timeit sgdr.fit(xtrain, ytrain)\n",
    "# score = sgdr.score(xtrain, ytrain)\n",
    "# print(\"R-squared:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [575990, 100000]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimeit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msgdr.fit(x, y)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\agodsenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2369\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2367\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[0;32m   2368\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m-> 2369\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\agodsenv\\lib\\site-packages\\IPython\\core\\magics\\execution.py:1162\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m   1161\u001b[0m     number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m index\n\u001b[1;32m-> 1162\u001b[0m     time_number \u001b[38;5;241m=\u001b[39m \u001b[43mtimer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time_number \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m:\n\u001b[0;32m   1164\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\agodsenv\\lib\\site-packages\\IPython\\core\\magics\\execution.py:156\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    154\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[1;32m<magic-timeit>:1\u001b[0m, in \u001b[0;36minner\u001b[1;34m(_it, _timer)\u001b[0m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\agodsenv\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1537\u001b[0m, in \u001b[0;36mBaseSGDRegressor.fit\u001b[1;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m   1512\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, coef_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, intercept_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1513\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit linear model with Stochastic Gradient Descent.\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m \n\u001b[0;32m   1515\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1535\u001b[0m \u001b[38;5;124;03m        Fitted `SGDRegressor` estimator.\u001b[39;00m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoef_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintercept_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintercept_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1547\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\agodsenv\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1485\u001b[0m, in \u001b[0;36mBaseSGDRegressor._fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m   1482\u001b[0m \u001b[38;5;66;03m# Clear iteration count for multiple call to fit.\u001b[39;00m\n\u001b[0;32m   1483\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m-> 1485\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_partial_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintercept_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1496\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1499\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1500\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf\n\u001b[0;32m   1501\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter\n\u001b[0;32m   1502\u001b[0m ):\n\u001b[0;32m   1503\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaximum number of iteration reached before \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1505\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvergence. Consider increasing max_iter to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1506\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimprove the fit.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1507\u001b[0m         ConvergenceWarning,\n\u001b[0;32m   1508\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\agodsenv\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1392\u001b[0m, in \u001b[0;36mBaseSGDRegressor._partial_fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_partial_fit\u001b[39m(\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1380\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1389\u001b[0m     intercept_init,\n\u001b[0;32m   1390\u001b[0m ):\n\u001b[0;32m   1391\u001b[0m     first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoef_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1392\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1394\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1395\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1397\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1399\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1401\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1402\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1404\u001b[0m     n_samples, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\agodsenv\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\agodsenv\\lib\\site-packages\\sklearn\\utils\\validation.py:981\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    964\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    965\u001b[0m     X,\n\u001b[0;32m    966\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    976\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    977\u001b[0m )\n\u001b[0;32m    979\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric)\n\u001b[1;32m--> 981\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\agodsenv\\lib\\site-packages\\sklearn\\utils\\validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    330\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    335\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [575990, 100000]"
     ]
    }
   ],
   "source": [
    "%timeit sgdr.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit lr.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit lr.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYwmyhrZXjJO"
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "In the practical, we looked at how to create a polymomial that fits a set of data using the least square method. The polynomial is then displayed as the best-fit line.\n",
    "\n",
    "We also see how to use Scikit-Learn to generate Linear and Regression models (both for simple and mulitple variables).  We have also seen how to evaluate the models using the MSE, R squared and adjusted R squared values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2021 LinearRegression v1.0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
